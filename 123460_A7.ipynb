{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1f7cf1fd444400ab704831b09896764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1184dae1128a428ba97997813cb62f3d",
              "IPY_MODEL_8467bd5dabea40f591805a4d7f0f7555",
              "IPY_MODEL_0763426b70ab4f11b78d206027dba741"
            ],
            "layout": "IPY_MODEL_4aae9d9526d3495db3fdc08d56fa447c"
          }
        },
        "1184dae1128a428ba97997813cb62f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcf8db44572743f7bef258d8854e0667",
            "placeholder": "​",
            "style": "IPY_MODEL_1b5a4e572c504eb88fe23cb683ea4efb",
            "value": "100%"
          }
        },
        "8467bd5dabea40f591805a4d7f0f7555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e829f497f4cf47559b7b568929ddc207",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bedcdac82a7a453fb2f4b47f3c8d4d5e",
            "value": 2
          }
        },
        "0763426b70ab4f11b78d206027dba741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc292f58b93641e8b6598cc3e44a320e",
            "placeholder": "​",
            "style": "IPY_MODEL_2198358ed0c6438384a437772dedaf45",
            "value": " 2/2 [00:00&lt;00:00, 75.75it/s]"
          }
        },
        "4aae9d9526d3495db3fdc08d56fa447c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf8db44572743f7bef258d8854e0667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b5a4e572c504eb88fe23cb683ea4efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e829f497f4cf47559b7b568929ddc207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bedcdac82a7a453fb2f4b47f3c8d4d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc292f58b93641e8b6598cc3e44a320e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2198358ed0c6438384a437772dedaf45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fizzyf0xy/NLP/blob/main/123460_A7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notation:\n",
        "I do this assignment together with Tan and Maria"
      ],
      "metadata": {
        "id": "dACYkESa0kvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fhGtnLGnCl7",
        "outputId": "0dfd4acb-af51-4ebb-8902-a73c73790709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKM2xKbhYIbi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchtext, datasets, math\n",
        "from tqdm import tqdm\n",
        "\n",
        "from queue import PriorityQueue\n",
        "import operator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "#make our work comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "# torch.manual_seed(SEED)\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mESHLdbJYsX5",
        "outputId": "36e426a5-5b8d-4fcb-fc72-4ff68c1ddaf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load data - Python"
      ],
      "metadata": {
        "id": "EXLFbwwaYzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#there are raw and preprocessed version; we used the raw one and preprocessed ourselves for fun\n",
        "dataset = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "d1f7cf1fd444400ab704831b09896764",
            "1184dae1128a428ba97997813cb62f3d",
            "8467bd5dabea40f591805a4d7f0f7555",
            "0763426b70ab4f11b78d206027dba741",
            "4aae9d9526d3495db3fdc08d56fa447c",
            "bcf8db44572743f7bef258d8854e0667",
            "1b5a4e572c504eb88fe23cb683ea4efb",
            "e829f497f4cf47559b7b568929ddc207",
            "bedcdac82a7a453fb2f4b47f3c8d4d5e",
            "dc292f58b93641e8b6598cc3e44a320e",
            "2198358ed0c6438384a437772dedaf45"
          ]
        },
        "id": "pW2WpRWxYxbs",
        "outputId": "ac99cd73-fe68-44f1-dc20-96f275584bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1f7cf1fd444400ab704831b09896764"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['repo_name', 'path', 'license', 'content'],\n",
            "        num_rows: 47452\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['repo_name', 'path', 'license', 'content'],\n",
            "        num_rows: 11864\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][1].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ8mpUChYuue",
        "outputId": "bef4e9f5-5da4-4b5e-ee6b-29751a705f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['repo_name', 'path', 'license', 'content'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing"
      ],
      "metadata": {
        "id": "RicTnUImZJvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "\n",
        "#function to tokenize\n",
        "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['content'])}  \n",
        "\n",
        "#map the function to each example\n",
        "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['content'], fn_kwargs={'tokenizer': tokenizer})\n",
        "print(tokenized_dataset['train'][333]['tokens'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef2bXPh6ZHfd",
        "outputId": "7a2eb117-0cf3-4d49-c9a8-19b82a411a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-72f86021443be4cb.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-43058b0e23f14b65.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['import', 'hashlib', 'import', 'os', 'import', 'pickle', 'from', 'urllib', '.', 'request', 'import', 'urlretrieve', 'import', 'numpy', 'as', 'np', 'from', 'pil', 'import', 'image', 'from', 'sklearn', '.', 'model_selection', 'import', 'train_test_split', 'from', 'sklearn', '.', 'preprocessing', 'import', 'labelbinarizer', 'from', 'sklearn', '.', 'utils', 'import', 'resample', 'from', 'tqdm', 'import', 'tqdm', 'from', 'zipfile', 'import', 'zipfile', 'print', '(', \"'\", 'all', 'modules', 'imported', '.', \"'\", ')', 'explanation', '<h1', 'align=center>tensorflow', 'neural', 'network', 'lab</h1>', '<img', 'src=image/notmnist', '.', 'png>', 'in', 'this', 'lab', ',', 'you', \"'\", 'll', 'use', 'all', 'the', 'tools', 'you', 'learned', 'from', 'introduction', 'to', 'tensorflow', 'to', 'label', 'images', 'of', 'english', 'letters', '!', 'the', 'data', 'you', 'are', 'using', ',', '<a', 'href=http', '//yaroslavvb', '.', 'blogspot', '.', 'com/2011/09/notmnist-dataset', '.', 'html>notmnist</a>', ',', 'consists', 'of', 'images', 'of', 'a', 'letter', 'from', 'a', 'to', 'j', 'in', 'different', 'fonts', '.', 'the', 'above', 'images', 'are', 'a', 'few', 'examples', 'of', 'the', 'data', 'you', \"'\", 'll', 'be', 'training', 'on', '.', 'after', 'training', 'the', 'network', ',', 'you', 'will', 'compare', 'your', 'prediction', 'model', 'against', 'test', 'data', '.', 'your', 'goal', ',', 'by', 'the', 'end', 'of', 'this', 'lab', ',', 'is', 'to', 'make', 'predictions', 'against', 'that', 'test', 'set', 'with', 'at', 'least', 'an', '80%', 'accuracy', '.', 'let', \"'\", 's', 'jump', 'in', '!', 'to', 'start', 'this', 'lab', ',', 'you', 'first', 'need', 'to', 'import', 'all', 'the', 'necessary', 'modules', '.', 'run', 'the', 'code', 'below', '.', 'if', 'it', 'runs', 'successfully', ',', 'it', 'will', 'print', 'all', 'modules', 'imported', '.', 'end', 'of', 'explanation', 'def', 'download', '(', 'url', ',', 'file', ')', 'download', 'file', 'from', '<url>', 'param', 'url', 'url', 'to', 'file', 'param', 'file', 'local', 'file', 'path', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'file', ')', 'print', '(', \"'\", 'downloading', \"'\", '+', 'file', '+', \"'\", '.', '.', '.', \"'\", ')', 'urlretrieve', '(', 'url', ',', 'file', ')', 'print', '(', \"'\", 'download', 'finished', \"'\", ')', '#', 'download', 'the', 'training', 'and', 'test', 'dataset', '.', 'download', '(', \"'\", 'https', '//s3', '.', 'amazonaws', '.', 'com/udacity-sdc/notmnist_train', '.', 'zip', \"'\", ',', \"'\", 'notmnist_train', '.', 'zip', \"'\", ')', 'download', '(', \"'\", 'https', '//s3', '.', 'amazonaws', '.', 'com/udacity-sdc/notmnist_test', '.', 'zip', \"'\", ',', \"'\", 'notmnist_test', '.', 'zip', \"'\", ')', '#', 'make', 'sure', 'the', 'files', 'aren', \"'\", 't', 'corrupted', 'assert', 'hashlib', '.', 'md5', '(', 'open', '(', \"'\", 'notmnist_train', '.', 'zip', \"'\", ',', \"'\", 'rb', \"'\", ')', '.', 'read', '(', ')', ')', '.', 'hexdigest', '(', ')', '==', \"'\", 'c8673b3f28f489e9cdf3a3d74e2ac8fa', \"'\", ',', '\\\\', \"'\", 'notmnist_train', '.', 'zip', 'file', 'is', 'corrupted', '.', 'remove', 'the', 'file', 'and', 'try', 'again', '.', \"'\", 'assert', 'hashlib', '.', 'md5', '(', 'open', '(', \"'\", 'notmnist_test', '.', 'zip', \"'\", ',', \"'\", 'rb', \"'\", ')', '.', 'read', '(', ')', ')', '.', 'hexdigest', '(', ')', '==', \"'\", '5d3c7e653e63471c88df796156a9dfa9', \"'\", ',', '\\\\', \"'\", 'notmnist_test', '.', 'zip', 'file', 'is', 'corrupted', '.', 'remove', 'the', 'file', 'and', 'try', 'again', '.', \"'\", '#', 'wait', 'until', 'you', 'see', 'that', 'all', 'files', 'have', 'been', 'downloaded', '.', 'print', '(', \"'\", 'all', 'files', 'downloaded', '.', \"'\", ')', 'def', 'uncompress_features_labels', '(', 'file', ')', 'uncompress', 'features', 'and', 'labels', 'from', 'a', 'zip', 'file', 'param', 'file', 'the', 'zip', 'file', 'to', 'extract', 'the', 'data', 'from', 'features', '=', '[]', 'labels', '=', '[]', 'with', 'zipfile', '(', 'file', ')', 'as', 'zipf', '#', 'progress', 'bar', 'filenames_pbar', '=', 'tqdm', '(', 'zipf', '.', 'namelist', '(', ')', ',', 'unit=', \"'\", 'files', \"'\", ')', '#', 'get', 'features', 'and', 'labels', 'from', 'all', 'files', 'for', 'filename', 'in', 'filenames_pbar', '#', 'check', 'if', 'the', 'file', 'is', 'a', 'directory', 'if', 'not', 'filename', '.', 'endswith', '(', \"'\", '/', \"'\", ')', 'with', 'zipf', '.', 'open', '(', 'filename', ')', 'as', 'image_file', 'image', '=', 'image', '.', 'open', '(', 'image_file', ')', 'image', '.', 'load', '(', ')', '#', 'load', 'image', 'data', 'as', '1', 'dimensional', 'array', '#', 'we', \"'\", 're', 'using', 'float32', 'to', 'save', 'on', 'memory', 'space', 'feature', '=', 'np', '.', 'array', '(', 'image', ',', 'dtype=np', '.', 'float32', ')', '.', 'flatten', '(', ')', '#', 'get', 'the', 'the', 'letter', 'from', 'the', 'filename', '.', 'this', 'is', 'the', 'letter', 'of', 'the', 'image', '.', 'label', '=', 'os', '.', 'path', '.', 'split', '(', 'filename', ')', '[1][0]', 'features', '.', 'append', '(', 'feature', ')', 'labels', '.', 'append', '(', 'label', ')', 'return', 'np', '.', 'array', '(', 'features', ')', ',', 'np', '.', 'array', '(', 'labels', ')', '#', 'get', 'the', 'features', 'and', 'labels', 'from', 'the', 'zip', 'files', 'train_features', ',', 'train_labels', '=', 'uncompress_features_labels', '(', \"'\", 'notmnist_train', '.', 'zip', \"'\", ')', 'test_features', ',', 'test_labels', '=', 'uncompress_features_labels', '(', \"'\", 'notmnist_test', '.', 'zip', \"'\", ')', '#', 'limit', 'the', 'amount', 'of', 'data', 'to', 'work', 'with', 'a', 'docker', 'container', 'docker_size_limit', '=', '150000', 'train_features', ',', 'train_labels', '=', 'resample', '(', 'train_features', ',', 'train_labels', ',', 'n_samples=docker_size_limit', ')', '#', 'set', 'flags', 'for', 'feature', 'engineering', '.', 'this', 'will', 'prevent', 'you', 'from', 'skipping', 'an', 'important', 'step', '.', 'is_features_normal', '=', 'false', 'is_labels_encod', '=', 'false', '#', 'wait', 'until', 'you', 'see', 'that', 'all', 'features', 'and', 'labels', 'have', 'been', 'uncompressed', '.', 'print', '(', \"'\", 'all', 'features', 'and', 'labels', 'uncompressed', '.', \"'\", ')', 'explanation', 'the', 'notmnist', 'dataset', 'is', 'too', 'large', 'for', 'many', 'computers', 'to', 'handle', '.', 'it', 'contains', '500', ',', '000', 'images', 'for', 'just', 'training', '.', 'you', \"'\", 'll', 'be', 'using', 'a', 'subset', 'of', 'this', 'data', ',', '15', ',', '000', 'images', 'for', 'each', 'label', '(', 'a-j', ')', '.', 'end', 'of', 'explanation', '#', 'problem', '1', '-', 'implement', 'min-max', 'scaling', 'for', 'grayscale', 'image', 'data', 'def', 'normalize_grayscale', '(', 'image_data', ')', 'normalize', 'the', 'image', 'data', 'with', 'min-max', 'scaling', 'to', 'a', 'range', 'of', '[0', '.', '1', ',', '0', '.', '9]', 'param', 'image_data', 'the', 'image', 'data', 'to', 'be', 'normalized', 'return', 'normalized', 'image', 'data', 'arr', '=', 'np', '.', 'array', '(', 'image_data', ')', 'arr', '=', '(', 'arr', '-', 'arr', '.', 'min', '(', ')', ')', '/', '(', 'arr', '.', 'max', '(', ')', '-', 'arr', '.', 'min', '(', ')', ')', 'arr', '=', 'arr', '*', '0', '.', '8', '+', '0', '.', '1', 'return', 'arr', '.', 'tolist', '(', ')', '###', 'don', \"'\", 't', 'modify', 'anything', 'below', '###', '#', 'test', 'cases', 'np', '.', 'testing', '.', 'assert_array_almost_equal', '(', 'normalize_grayscale', '(', 'np', '.', 'array', '(', '[0', ',', '1', ',', '2', ',', '3', ',', '4', ',', '5', ',', '6', ',', '7', ',', '8', ',', '9', ',', '10', ',', '255]', ')', ')', ',', '[0', '.', '1', ',', '0', '.', '103137254902', ',', '0', '.', '106274509804', ',', '0', '.', '109411764706', ',', '0', '.', '112549019608', ',', '0', '.', '11568627451', ',', '0', '.', '118823529412', ',', '0', '.', '121960784314', ',', '0', '.', '125098039216', ',', '0', '.', '128235294118', ',', '0', '.', '13137254902', ',', '0', '.', '9]', ',', 'decimal=3', ')', 'np', '.', 'testing', '.', 'assert_array_almost_equal', '(', 'normalize_grayscale', '(', 'np', '.', 'array', '(', '[0', ',', '1', ',', '10', ',', '20', ',', '30', ',', '40', ',', '233', ',', '244', ',', '254', ',', '255]', ')', ')', ',', '[0', '.', '1', ',', '0', '.', '103137254902', ',', '0', '.', '13137254902', ',', '0', '.', '162745098039', ',', '0', '.', '194117647059', ',', '0', '.', '225490196078', ',', '0', '.', '830980392157', ',', '0', '.', '865490196078', ',', '0', '.', '896862745098', ',', '0', '.', '9]', ')', 'print', '(', 'train_features', ')', 'if', 'not', 'is_features_normal', 'train_features', '=', 'normalize_grayscale', '(', 'train_features', ')', 'test_features', '=', 'normalize_grayscale', '(', 'test_features', ')', 'is_features_normal', '=', 'true', 'print', '(', \"'\", 'tests', 'passed', '!', \"'\", ')', 'if', 'not', 'is_labels_encod', '#', 'turn', 'labels', 'into', 'numbers', 'and', 'apply', 'one-hot', 'encoding', 'encoder', '=', 'labelbinarizer', '(', ')', 'encoder', '.', 'fit', '(', 'train_labels', ')', 'train_labels', '=', 'encoder', '.', 'transform', '(', 'train_labels', ')', 'test_labels', '=', 'encoder', '.', 'transform', '(', 'test_labels', ')', '#', 'change', 'to', 'float32', ',', 'so', 'it', 'can', 'be', 'multiplied', 'against', 'the', 'features', 'in', 'tensorflow', ',', 'which', 'are', 'float32', 'train_labels', '=', 'train_labels', '.', 'astype', '(', 'np', '.', 'float32', ')', 'test_labels', '=', 'test_labels', '.', 'astype', '(', 'np', '.', 'float32', ')', 'is_labels_encod', '=', 'true', 'print', '(', \"'\", 'labels', 'one-hot', 'encoded', \"'\", ')', 'assert', 'is_features_normal', ',', \"'\", 'you', 'skipped', 'the', 'step', 'to', 'normalize', 'the', 'features', \"'\", 'assert', 'is_labels_encod', ',', \"'\", 'you', 'skipped', 'the', 'step', 'to', 'one-hot', 'encode', 'the', 'labels', \"'\", '#', 'get', 'randomized', 'datasets', 'for', 'training', 'and', 'validation', 'train_features', ',', 'valid_features', ',', 'train_labels', ',', 'valid_labels', '=', 'train_test_split', '(', 'train_features', ',', 'train_labels', ',', 'test_size=0', '.', '05', ',', 'random_state=832289', ')', 'print', '(', \"'\", 'training', 'features', 'and', 'labels', 'randomized', 'and', 'split', '.', \"'\", ')', '#', 'save', 'the', 'data', 'for', 'easy', 'access', 'pickle_file', '=', \"'\", 'notmnist', '.', 'pickle', \"'\", 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'pickle_file', ')', 'print', '(', \"'\", 'saving', 'data', 'to', 'pickle', 'file', '.', '.', '.', \"'\", ')', 'try', 'with', 'open', '(', \"'\", 'notmnist', '.', 'pickle', \"'\", ',', \"'\", 'wb', \"'\", ')', 'as', 'pfile', 'pickle', '.', 'dump', '(', '{', \"'\", 'train_dataset', \"'\", 'train_features', ',', \"'\", 'train_labels', \"'\", 'train_labels', ',', \"'\", 'valid_dataset', \"'\", 'valid_features', ',', \"'\", 'valid_labels', \"'\", 'valid_labels', ',', \"'\", 'test_dataset', \"'\", 'test_features', ',', \"'\", 'test_labels', \"'\", 'test_labels', ',', '}', ',', 'pfile', ',', 'pickle', '.', 'highest_protocol', ')', 'except', 'exception', 'as', 'e', 'print', '(', \"'\", 'unable', 'to', 'save', 'data', 'to', \"'\", ',', 'pickle_file', ',', \"'\", \"'\", ',', 'e', ')', 'raise', 'print', '(', \"'\", 'data', 'cached', 'in', 'pickle', 'file', '.', \"'\", ')', 'explanation', '<img', 'src=image/mean_variance_image', '.', 'png', 'style=height', '75%', 'width', '75%', 'position', 'relative', 'right', '5%>', 'problem', '1', 'the', 'first', 'problem', 'involves', 'normalizing', 'the', 'features', 'for', 'your', 'training', 'and', 'test', 'data', '.', 'implement', 'min-max', 'scaling', 'in', 'the', 'normalize_grayscale', '(', ')', 'function', 'to', 'a', 'range', 'of', 'a=0', '.', '1', 'and', 'b=0', '.', '9', '.', 'after', 'scaling', ',', 'the', 'values', 'of', 'the', 'pixels', 'in', 'the', 'input', 'data', 'should', 'range', 'from', '0', '.', '1', 'to', '0', '.', '9', '.', 'since', 'the', 'raw', 'notmnist', 'image', 'data', 'is', 'in', 'grayscale', ',', 'the', 'current', 'values', 'range', 'from', 'a', 'min', 'of', '0', 'to', 'a', 'max', 'of', '255', '.', 'min-max', 'scaling', '$', 'x', \"'\", '=a+{\\\\frac', '{\\\\left', '(', 'x-x_{\\\\min', '}\\\\right', ')', '\\\\left', '(', 'b-a\\\\right', ')', '}{x_{\\\\max', '}-x_{\\\\min', '}}}', '$', 'if', 'you', \"'\", 're', 'having', 'trouble', 'solving', 'problem', '1', ',', 'you', 'can', 'view', 'the', 'solution', 'here', '.', 'end', 'of', 'explanation', '%matplotlib', 'inline', '#', 'load', 'the', 'modules', 'import', 'pickle', 'import', 'math', 'import', 'numpy', 'as', 'np', 'import', 'tensorflow', 'as', 'tf', 'from', 'tqdm', 'import', 'tqdm', 'import', 'matplotlib', '.', 'pyplot', 'as', 'plt', '#', 'reload', 'the', 'data', 'pickle_file', '=', \"'\", 'notmnist', '.', 'pickle', \"'\", 'with', 'open', '(', 'pickle_file', ',', \"'\", 'rb', \"'\", ')', 'as', 'f', 'pickle_data', '=', 'pickle', '.', 'load', '(', 'f', ')', 'train_features', '=', 'pickle_data[', \"'\", 'train_dataset', \"'\", ']', 'train_labels', '=', 'pickle_data[', \"'\", 'train_labels', \"'\", ']', 'valid_features', '=', 'pickle_data[', \"'\", 'valid_dataset', \"'\", ']', 'valid_labels', '=', 'pickle_data[', \"'\", 'valid_labels', \"'\", ']', 'test_features', '=', 'pickle_data[', \"'\", 'test_dataset', \"'\", ']', 'test_labels', '=', 'pickle_data[', \"'\", 'test_labels', \"'\", ']', 'del', 'pickle_data', '#', 'free', 'up', 'memory', 'print', '(', \"'\", 'data', 'and', 'modules', 'loaded', '.', \"'\", ')', 'explanation', 'checkpoint', 'all', 'your', 'progress', 'is', 'now', 'saved', 'to', 'the', 'pickle', 'file', '.', 'if', 'you', 'need', 'to', 'leave', 'and', 'comeback', 'to', 'this', 'lab', ',', 'you', 'no', 'longer', 'have', 'to', 'start', 'from', 'the', 'beginning', '.', 'just', 'run', 'the', 'code', 'block', 'below', 'and', 'it', 'will', 'load', 'all', 'the', 'data', 'and', 'modules', 'required', 'to', 'proceed', '.', 'end', 'of', 'explanation', '#', 'all', 'the', 'pixels', 'in', 'the', 'image', '(', '28', '*', '28', '=', '784', ')', 'features_count', '=', '784', '#', 'all', 'the', 'labels', 'labels_count', '=', '10', 'features', '=', 'tf', '.', 'placeholder', '(', 'tf', '.', 'float32', ',', '[none', ',', 'features_count]', ')', 'labels', '=', 'tf', '.', 'placeholder', '(', 'tf', '.', 'float32', ',', '[none', ',', 'labels_count]', ')', 'weights', '=', 'tf', '.', 'variable', '(', 'tf', '.', 'truncated_normal', '(', '[features_count', ',', 'labels_count]', ')', ')', 'biases', '=', 'tf', '.', 'variable', '(', 'tf', '.', 'zeros', '(', '[labels_count]', ')', ')', '###', 'don', \"'\", 't', 'modify', 'anything', 'below', '###', '#test', 'cases', 'from', 'tensorflow', '.', 'python', '.', 'ops', '.', 'variables', 'import', 'variable', 'assert', 'features', '.', '_op', '.', 'name', '.', 'startswith', '(', \"'\", 'placeholder', \"'\", ')', ',', \"'\", 'features', 'must', 'be', 'a', 'placeholder', \"'\", 'assert', 'labels', '.', '_op', '.', 'name', '.', 'startswith', '(', \"'\", 'placeholder', \"'\", ')', ',', \"'\", 'labels', 'must', 'be', 'a', 'placeholder', \"'\", 'assert', 'isinstance', '(', 'weights', ',', 'variable', ')', ',', \"'\", 'weights', 'must', 'be', 'a', 'tensorflow', 'variable', \"'\", 'assert', 'isinstance', '(', 'biases', ',', 'variable', ')', ',', \"'\", 'biases', 'must', 'be', 'a', 'tensorflow', 'variable', \"'\", 'assert', 'features', '.', '_shape', '==', 'none', 'or', '(', '\\\\', 'features', '.', '_shape', '.', 'dims[0]', '.', 'value', 'is', 'none', 'and\\\\', 'features', '.', '_shape', '.', 'dims[1]', '.', 'value', 'in', '[none', ',', '784]', ')', ',', \"'\", 'the', 'shape', 'of', 'features', 'is', 'incorrect', \"'\", 'assert', 'labels', '.', '_shape', '==', 'none', 'or', '(', '\\\\', 'labels', '.', '_shape', '.', 'dims[0]', '.', 'value', 'is', 'none', 'and\\\\', 'labels', '.', '_shape', '.', 'dims[1]', '.', 'value', 'in', '[none', ',', '10]', ')', ',', \"'\", 'the', 'shape', 'of', 'labels', 'is', 'incorrect', \"'\", 'assert', 'weights', '.', '_variable', '.', '_shape', '==', '(', '784', ',', '10', ')', ',', \"'\", 'the', 'shape', 'of', 'weights', 'is', 'incorrect', \"'\", 'assert', 'biases', '.', '_variable', '.', '_shape', '==', '(', '10', ')', ',', \"'\", 'the', 'shape', 'of', 'biases', 'is', 'incorrect', \"'\", 'assert', 'features', '.', '_dtype', '==', 'tf', '.', 'float32', ',', \"'\", 'features', 'must', 'be', 'type', 'float32', \"'\", 'assert', 'labels', '.', '_dtype', '==', 'tf', '.', 'float32', ',', \"'\", 'labels', 'must', 'be', 'type', 'float32', \"'\", '#', 'feed', 'dicts', 'for', 'training', ',', 'validation', ',', 'and', 'test', 'session', 'train_feed_dict', '=', '{features', 'train_features', ',', 'labels', 'train_labels}', 'valid_feed_dict', '=', '{features', 'valid_features', ',', 'labels', 'valid_labels}', 'test_feed_dict', '=', '{features', 'test_features', ',', 'labels', 'test_labels}', '#', 'linear', 'function', 'wx', '+', 'b', 'logits', '=', 'tf', '.', 'matmul', '(', 'features', ',', 'weights', ')', '+', 'biases', 'prediction', '=', 'tf', '.', 'nn', '.', 'softmax', '(', 'logits', ')', '#', 'cross', 'entropy', 'cross_entropy', '=', '-tf', '.', 'reduce_sum', '(', 'labels', '*', 'tf', '.', 'log', '(', 'prediction', ')', ',', 'reduction_indices=1', ')', '#', 'training', 'loss', 'loss', '=', 'tf', '.', 'reduce_mean', '(', 'cross_entropy', ')', '#', 'create', 'an', 'operation', 'that', 'initializes', 'all', 'variables', 'init', '=', 'tf', '.', 'global_variables_initializer', '(', ')', '#', 'test', 'cases', 'with', 'tf', '.', 'session', '(', ')', 'as', 'session', 'session', '.', 'run', '(', 'init', ')', 'session', '.', 'run', '(', 'loss', ',', 'feed_dict=train_feed_dict', ')', 'session', '.', 'run', '(', 'loss', ',', 'feed_dict=valid_feed_dict', ')', 'session', '.', 'run', '(', 'loss', ',', 'feed_dict=test_feed_dict', ')', 'biases_data', '=', 'session', '.', 'run', '(', 'biases', ')', 'assert', 'not', 'np', '.', 'count_nonzero', '(', 'biases_data', ')', ',', \"'\", 'biases', 'must', 'be', 'zeros', \"'\", 'print', '(', \"'\", 'tests', 'passed', '!', \"'\", ')', '#', 'determine', 'if', 'the', 'predictions', 'are', 'correct', 'is_correct_prediction', '=', 'tf', '.', 'equal', '(', 'tf', '.', 'argmax', '(', 'prediction', ',', '1', ')', ',', 'tf', '.', 'argmax', '(', 'labels', ',', '1', ')', ')', '#', 'calculate', 'the', 'accuracy', 'of', 'the', 'predictions', 'accuracy', '=', 'tf', '.', 'reduce_mean', '(', 'tf', '.', 'cast', '(', 'is_correct_prediction', ',', 'tf', '.', 'float32', ')', ')', 'print', '(', \"'\", 'accuracy', 'function', 'created', '.', \"'\", ')', 'explanation', 'problem', '2', 'now', 'it', \"'\", 's', 'time', 'to', 'build', 'a', 'simple', 'neural', 'network', 'using', 'tensorflow', '.', 'here', ',', 'your', 'network', 'will', 'be', 'just', 'an', 'input', 'layer', 'and', 'an', 'output', 'layer', '.', '<img', 'src=image/network_diagram', '.', 'png', 'style=height', '40%', 'width', '40%', 'position', 'relative', 'right', '10%>', 'for', 'the', 'input', 'here', 'the', 'images', 'have', 'been', 'flattened', 'into', 'a', 'vector', 'of', '$28', '\\\\times', '28', '=', '784$', 'features', '.', 'then', ',', 'we', \"'\", 're', 'trying', 'to', 'predict', 'the', 'image', 'digit', 'so', 'there', 'are', '10', 'output', 'units', ',', 'one', 'for', 'each', 'label', '.', 'of', 'course', ',', 'feel', 'free', 'to', 'add', 'hidden', 'layers', 'if', 'you', 'want', ',', 'but', 'this', 'notebook', 'is', 'built', 'to', 'guide', 'you', 'through', 'a', 'single', 'layer', 'network', '.', 'for', 'the', 'neural', 'network', 'to', 'train', 'on', 'your', 'data', ',', 'you', 'need', 'the', 'following', '<a', 'href=https', '//www', '.', 'tensorflow', '.', 'org/resources/dims_types', '.', 'html#data-types>float32</a>', 'tensors', '-', 'features', '-', 'placeholder', 'tensor', 'for', 'feature', 'data', '(', 'train_features/valid_features/test_features', ')', '-', 'labels', '-', 'placeholder', 'tensor', 'for', 'label', 'data', '(', 'train_labels/valid_labels/test_labels', ')', '-', 'weights', '-', 'variable', 'tensor', 'with', 'random', 'numbers', 'from', 'a', 'truncated', 'normal', 'distribution', '.', '-', 'see', '<a', 'href=https', '//www', '.', 'tensorflow', '.', 'org/api_docs/python/constant_op', '.', 'html#truncated_normal>tf', '.', 'truncated_normal', '(', ')', 'documentation</a>', 'for', 'help', '.', '-', 'biases', '-', 'variable', 'tensor', 'with', 'all', 'zeros', '.', '-', 'see', '<a', 'href=https', '//www', '.', 'tensorflow', '.', 'org/api_docs/python/constant_op', '.', 'html#zeros>', 'tf', '.', 'zeros', '(', ')', 'documentation</a>', 'for', 'help', '.', 'if', 'you', \"'\", 're', 'having', 'trouble', 'solving', 'problem', '2', ',', 'review', 'tensorflow', 'linear', 'function', 'section', 'of', 'the', 'class', '.', 'if', 'that', 'doesn', \"'\", 't', 'help', ',', 'the', 'solution', 'for', 'this', 'problem', 'is', 'available', 'here', '.', 'end', 'of', 'explanation', 'results', '#', 'change', 'if', 'you', 'have', 'memory', 'restrictions', 'batch_size', '=', '128', '#', 'find', 'the', 'best', 'parameters', 'for', 'each', 'configuration', '#results', '=', '[]', '#for', 'epochs', 'in', '[1', ',', '2', ',', '3', ',', '4', ',', '5]', '#', 'for', 'learning_rate', 'in', '[0', '.', '8', ',', '0', '.', '5', ',', '0', '.', '1', ',', '0', '.', '05', ',', '0', '.', '01]', 'epochs', '=', '4', 'learning_rate', '=', '0', '.', '1', '###', 'don', \"'\", 't', 'modify', 'anything', 'below', '###', '#', 'gradient', 'descent', 'optimizer', '=', 'tf', '.', 'train', '.', 'gradientdescentoptimizer', '(', 'learning_rate', ')', '.', 'minimize', '(', 'loss', ')', '#', 'the', 'accuracy', 'measured', 'against', 'the', 'validation', 'set', 'validation_accuracy', '=', '0', '.', '0', '#', 'measurements', 'use', 'for', 'graphing', 'loss', 'and', 'accuracy', 'log_batch_step', '=', '50', 'batches', '=', '[]', 'loss_batch', '=', '[]', 'train_acc_batch', '=', '[]', 'valid_acc_batch', '=', '[]', 'with', 'tf', '.', 'session', '(', ')', 'as', 'session', 'session', '.', 'run', '(', 'init', ')', 'batch_count', '=', 'int', '(', 'math', '.', 'ceil', '(', 'len', '(', 'train_features', ')', '/batch_size', ')', ')', 'for', 'epoch_i', 'in', 'range', '(', 'epochs', ')', '#', 'progress', 'bar', 'batches_pbar', '=', 'tqdm', '(', 'range', '(', 'batch_count', ')', ',', 'desc=', \"'\", 'epoch', '{', '>2}/{}', \"'\", '.', 'format', '(', 'epoch_i+1', ',', 'epochs', ')', ',', 'unit=', \"'\", 'batches', \"'\", ')', '#', 'the', 'training', 'cycle', 'for', 'batch_i', 'in', 'batches_pbar', '#', 'get', 'a', 'batch', 'of', 'training', 'features', 'and', 'labels', 'batch_start', '=', 'batch_i*batch_size', 'batch_features', '=', 'train_features[batch_start', 'batch_start', '+', 'batch_size]', 'batch_labels', '=', 'train_labels[batch_start', 'batch_start', '+', 'batch_size]', '#', 'run', 'optimizer', 'and', 'get', 'loss', '_', ',', 'l', '=', 'session', '.', 'run', '(', '[optimizer', ',', 'loss]', ',', 'feed_dict={features', 'batch_features', ',', 'labels', 'batch_labels}', ')', '#', 'log', 'every', '50', 'batches', 'if', 'not', 'batch_i', '%', 'log_batch_step', '#', 'calculate', 'training', 'and', 'validation', 'accuracy', 'training_accuracy', '=', 'session', '.', 'run', '(', 'accuracy', ',', 'feed_dict=train_feed_dict', ')', 'validation_accuracy', '=', 'session', '.', 'run', '(', 'accuracy', ',', 'feed_dict=valid_feed_dict', ')', '#', 'log', 'batches', 'previous_batch', '=', 'batches[-1]', 'if', 'batches', 'else', '0', 'batches', '.', 'append', '(', 'log_batch_step', '+', 'previous_batch', ')', 'loss_batch', '.', 'append', '(', 'l', ')', 'train_acc_batch', '.', 'append', '(', 'training_accuracy', ')', 'valid_acc_batch', '.', 'append', '(', 'validation_accuracy', ')', '#', 'check', 'accuracy', 'against', 'validation', 'data', 'validation_accuracy', '=', 'session', '.', 'run', '(', 'accuracy', ',', 'feed_dict=valid_feed_dict', ')', 'loss_plot', '=', 'plt', '.', 'subplot', '(', '211', ')', 'loss_plot', '.', 'set_title', '(', \"'\", 'loss', \"'\", ')', 'loss_plot', '.', 'plot', '(', 'batches', ',', 'loss_batch', ',', \"'\", 'g', \"'\", ')', 'loss_plot', '.', 'set_xlim', '(', '[batches[0]', ',', 'batches[-1]]', ')', 'acc_plot', '=', 'plt', '.', 'subplot', '(', '212', ')', 'acc_plot', '.', 'set_title', '(', \"'\", 'accuracy', \"'\", ')', 'acc_plot', '.', 'plot', '(', 'batches', ',', 'train_acc_batch', ',', \"'\", 'r', \"'\", ',', 'label=', \"'\", 'training', 'accuracy', \"'\", ')', 'acc_plot', '.', 'plot', '(', 'batches', ',', 'valid_acc_batch', ',', \"'\", 'x', \"'\", ',', 'label=', \"'\", 'validation', 'accuracy', \"'\", ')', 'acc_plot', '.', 'set_ylim', '(', '[0', ',', '1', '.', '0]', ')', 'acc_plot', '.', 'set_xlim', '(', '[batches[0]', ',', 'batches[-1]]', ')', 'acc_plot', '.', 'legend', '(', 'loc=4', ')', 'plt', '.', 'tight_layout', '(', ')', 'plt', '.', 'show', '(', ')', 'print', '(', \"'\", 'validation', 'accuracy', 'at', '{}', \"'\", '.', 'format', '(', 'validation_accuracy', ')', ')', '#print', '(', \"'\", 'lr', '{}\\\\t', 'epochs', '{}\\\\t', 'validation', 'accuracy', 'at', '{}', \"'\", '.', 'format', '(', 'learning_rate', ',', 'epochs', ',', 'validation_accuracy', ')', ')', '#results', '.', 'append', '(', '(', 'learning_rate', ',', 'epochs', ',', 'validation_accuracy', ')', ')', 'explanation', '<img', 'src=image/learn_rate_tune_image', '.', 'png', 'style=height', '70%', 'width', '70%>', 'problem', '3', 'below', 'are', '2', 'parameter', 'configurations', 'for', 'training', 'the', 'neural', 'network', '.', 'in', 'each', 'configuration', ',', 'one', 'of', 'the', 'parameters', 'has', 'multiple', 'options', '.', 'for', 'each', 'configuration', ',', 'choose', 'the', 'option', 'that', 'gives', 'the', 'best', 'acccuracy', '.', 'parameter', 'configurations', 'configuration', '1', '*', 'epochs', '1', '*', 'learning', 'rate', '*', '0', '.', '8', '*', '0', '.', '5', '*', '0', '.', '1', '*', '0', '.', '05', '*', '0', '.', '01', 'configuration', '2', '*', 'epochs', '*', '1', '*', '2', '*', '3', '*', '4', '*', '5', '*', 'learning', 'rate', '0', '.', '2', 'the', 'code', 'will', 'print', 'out', 'a', 'loss', 'and', 'accuracy', 'graph', ',', 'so', 'you', 'can', 'see', 'how', 'well', 'the', 'neural', 'network', 'performed', '.', 'if', 'you', \"'\", 're', 'having', 'trouble', 'solving', 'problem', '3', ',', 'you', 'can', 'view', 'the', 'solution', 'here', '.', 'end', 'of', 'explanation', '###', 'don', \"'\", 't', 'modify', 'anything', 'below', '###', '#', 'the', 'accuracy', 'measured', 'against', 'the', 'test', 'set', 'test_accuracy', '=', '0', '.', '0', 'with', 'tf', '.', 'session', '(', ')', 'as', 'session', 'session', '.', 'run', '(', 'init', ')', 'batch_count', '=', 'int', '(', 'math', '.', 'ceil', '(', 'len', '(', 'train_features', ')', '/batch_size', ')', ')', 'for', 'epoch_i', 'in', 'range', '(', 'epochs', ')', '#', 'progress', 'bar', 'batches_pbar', '=', 'tqdm', '(', 'range', '(', 'batch_count', ')', ',', 'desc=', \"'\", 'epoch', '{', '>2}/{}', \"'\", '.', 'format', '(', 'epoch_i+1', ',', 'epochs', ')', ',', 'unit=', \"'\", 'batches', \"'\", ')', '#', 'the', 'training', 'cycle', 'for', 'batch_i', 'in', 'batches_pbar', '#', 'get', 'a', 'batch', 'of', 'training', 'features', 'and', 'labels', 'batch_start', '=', 'batch_i*batch_size', 'batch_features', '=', 'train_features[batch_start', 'batch_start', '+', 'batch_size]', 'batch_labels', '=', 'train_labels[batch_start', 'batch_start', '+', 'batch_size]', '#', 'run', 'optimizer', '_', '=', 'session', '.', 'run', '(', 'optimizer', ',', 'feed_dict={features', 'batch_features', ',', 'labels', 'batch_labels}', ')', '#', 'check', 'accuracy', 'against', 'test', 'data', 'test_accuracy', '=', 'session', '.', 'run', '(', 'accuracy', ',', 'feed_dict=test_feed_dict', ')', 'assert', 'test_accuracy', '>=', '0', '.', '80', ',', \"'\", 'test', 'accuracy', 'at', '{}', ',', 'should', 'be', 'equal', 'to', 'or', 'greater', 'than', '0', '.', '80', \"'\", '.', 'format', '(', 'test_accuracy', ')', 'print', '(', \"'\", 'nice', 'job', '!', 'test', 'accuracy', 'is', '{}', \"'\", '.', 'format', '(', 'test_accuracy', ')', ')', 'explanation', 'test', 'you', \"'\", 're', 'going', 'to', 'test', 'your', 'model', 'against', 'your', 'hold', 'out', 'dataset/testing', 'data', '.', 'this', 'will', 'give', 'you', 'a', 'good', 'indicator', 'of', 'how', 'well', 'the', 'model', 'will', 'do', 'in', 'the', 'real', 'world', '.', 'you', 'should', 'have', 'a', 'test', 'accuracy', 'of', 'at', 'least', '80%', '.', 'end', 'of', 'explanation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## numericalizing\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
        "\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3, specials=special_symbols)   \n",
        "\n",
        "vocab.set_default_index(vocab['<unk>'])   \n",
        "print(len(vocab))                         \n",
        "print(vocab.get_itos()[:10])       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyUGOO0jZKbF",
        "outputId": "b773c6cc-9738-4ce5-a09c-da99994d2bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "611138\n",
            "['<unk>', '<pad>', '<sos>', '<eos>', '.', ',', ')', '(', \"'\", 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prepare the batch loader"
      ],
      "metadata": {
        "id": "ThJ3RriiZ6Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []                                                   \n",
        "    for example in dataset:\n",
        "        if example['tokens']:         \n",
        "            #appends eos so we know it ends....so model learn how to end...                             \n",
        "            tokens = example['tokens'].append('<eos>')   \n",
        "            #numericalize          \n",
        "            tokens = [vocab[token] for token in example['tokens']] \n",
        "            data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size #get the int number of batches...\n",
        "    data = data[:num_batches * batch_size] #make the batch evenly, and cut out any remaining                      \n",
        "    data = data.view(batch_size, num_batches)          \n",
        "    return data #[batch size, bunch of tokens]\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
        "valid_data = get_data(tokenized_dataset['test'], vocab, batch_size)"
      ],
      "metadata": {
        "id": "RhkLc-TVZ6ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modeling"
      ],
      "metadata": {
        "id": "DwbyoC0XaGxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "metadata": {
        "id": "MyMAgQGxaDEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "6sJVDdy6aPKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
        "                 pf_dim, dropout, device, pad_idx, max_length = 100):\n",
        "                \n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        self.pad_idx = pad_idx\n",
        "    \n",
        "    def make_mask(self, x):\n",
        "        \n",
        "        #x = [batch size, len]\n",
        "        \n",
        "        pad_mask = (x != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        #pad_mask = [batch size, 1, 1, len]\n",
        "        \n",
        "        x_len = x.shape[1]\n",
        "        \n",
        "        sub_mask = torch.tril(torch.ones((x_len, x_len), device = self.device)).bool()\n",
        "        #sub_mask = [len, len]\n",
        "            \n",
        "        mask = pad_mask & sub_mask\n",
        "        #mask = [batch size, 1, len, len]\n",
        "        \n",
        "        return mask \n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, len]\n",
        "                \n",
        "        batch_size = x.shape[0]\n",
        "        x_len      = x.shape[1]\n",
        "        \n",
        "        #get mask here since we remove seq2seq class\n",
        "        mask   = self.make_mask(x)\n",
        "        #mask = [batch size, 1, len, len]\n",
        "\n",
        "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
        "            \n",
        "        x = self.dropout((self.tok_embedding(x) * self.scale) + self.pos_embedding(pos))\n",
        "        #x = [batch size, len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            x, attention = layer(x, mask)\n",
        "        \n",
        "        #x = [batch size, len, hid dim]\n",
        "        #attention = [batch size, n heads, len, len]\n",
        "        \n",
        "        output = self.fc_out(x)\n",
        "        #output = [batch size, len, output dim]\n",
        "            \n",
        "        return output, attention\n",
        "\n",
        "    def beam_decode(self, penalty_alpha = 0.9, max_length = 5, beam_size = 5):\n",
        "        \n",
        "        # Start with SOS Harry Potter is\n",
        "        prompt = 'Harry Potter is '\n",
        "        \n",
        "        tokens = tokenizer(prompt)\n",
        "        indices = [SOS_IDX] + [vocab[t] for t in tokens]\n",
        "\n",
        "        decoder_input = torch.Tensor([indices]).long().to(device)\n",
        "        #decoder_input: [batch size, len] = [1, 1]\n",
        "        scores = torch.Tensor([0.]).to(device)\n",
        "        #scores: [1]\n",
        "        \n",
        "        for i in range(max_length):\n",
        "            \n",
        "            # print(f\"========Length: {i}\")\n",
        "            \n",
        "            # Decoder prediction\n",
        "            logits, _ = self.forward(decoder_input)\n",
        "            #[beam_size, current dec len=i, vocab_size]\n",
        "                        \n",
        "            logits = logits[:, -1] \n",
        "            # Last sequence step: [beam_size, current dec len=i, vocab_size] => [beam_size, vocab_size]\n",
        "            \n",
        "            # print(f\"{logits.shape=}\")\n",
        "\n",
        "            # Softmax\n",
        "            # Log softmax is better, since beam search accumulates probability\n",
        "            # if simply softmax, the probability can get too small and then become unstable\n",
        "            log_probs = torch.log_softmax(logits, dim=1)\n",
        "    \n",
        "            # Add length penalty, otherwise, always very short sentence will win...\n",
        "            penalty   = ((5 + (i+1)) / (5 + 1)) ** penalty_alpha #see https://arxiv.org/abs/1609.08144\n",
        "            log_probs = log_probs / penalty\n",
        "            \n",
        "            # print(f\"{decoder_input[:, -1]=}\")\n",
        "            \n",
        "            # Update score where EOS has not been reached\n",
        "            log_probs[decoder_input[:, -1]==EOS_IDX, :] = -2 #discouraged it to end\n",
        "            log_probs[decoder_input[:, -1]==UNK_IDX, :] = -10 #very discouraged to spit out unk\n",
        "            scores = scores.unsqueeze(1) + log_probs \n",
        "            # scores: [beam_size, vocab_size]\n",
        "            # log_probs: [beam_size, vocab_size]\n",
        "\n",
        "            # print(f\"{log_probs.shape=}\")\n",
        "            # print(f\"{scores.shape=}\")\n",
        "            #log_probs: torch.Size([1, 29475])\n",
        "            #scores.shape=torch.Size([1, 29475])\n",
        "            \n",
        "            # Flatten scores from [beams, vocab_size] to [beams * vocab_size] to get top k, and reconstruct beam indices and token indices\n",
        "            # Since we flatten it, we have to retrieve the actual beam indices and token_indices using floor division and remainder\n",
        "            # You can try on paper; it will make sense\n",
        "            scores, indices = torch.topk(scores.reshape(-1), beam_size) #scores: [beam_size]; #indices: [beam_size]\n",
        "            beam_indices  = torch.divide   (indices, self.output_dim, rounding_mode='floor') # indices // vocab_size\n",
        "            token_indices = torch.remainder(indices, self.output_dim)                        # indices %  vocab_size\n",
        "            \n",
        "            # print(f\"{scores=}\")\n",
        "            # print(f\"{indices.shape=}\")\n",
        "            \n",
        "            # print(f\"{indices=}\")\n",
        "            # print(f\"{beam_indices=}\")\n",
        "            # print(f\"{token_indices=}\")\n",
        "            \n",
        "            # Build the next decoder input\n",
        "            # For efficiency, the trick is to concatenate all hypotheses into one string and sent to decoder at once\n",
        "            # We can later chop it ...\n",
        "            next_decoder_input = []\n",
        "            for beam_index, token_index in zip(beam_indices, token_indices):\n",
        "                # print(f\"{beam_index=}\")\n",
        "                prev_decoder_input = decoder_input[beam_index]\n",
        "                # print(f\"{prev_decoder_input=}\")\n",
        "                if prev_decoder_input[-1]==EOS_IDX:\n",
        "                    token_index = EOS_IDX # once EOS, always EOS\n",
        "                token_index = torch.LongTensor([token_index]).long().to(device)\n",
        "                next_decoder_input.append(torch.cat([prev_decoder_input, token_index]))\n",
        "                # print(\"here: \" + \" \".join([vocab.lookup_token(i) for i in next_decoder_input[-1]]) + \"; score: \" + str(scores[beam_index].item()))\n",
        "            decoder_input = torch.vstack(next_decoder_input)\n",
        "            \n",
        "            # print(f\"{decoder_input=}\")\n",
        "            \n",
        "             # If all beams are finished, and the length is at least 5, exit\n",
        "            if i > 5:\n",
        "                if (decoder_input[:, -1]==EOS_IDX).sum() == beam_size:\n",
        "                    break\n",
        "                \n",
        "        # convert the top scored sequence to a list of text tokens\n",
        "        decoder_output, _ = max(zip(decoder_input, scores), key=lambda x: x[1])\n",
        "        decoder_output = decoder_output[1:].cpu().numpy() # remove SOS\n",
        "        \n",
        "        return [vocab.lookup_token(i) for i in decoder_output if i != EOS_IDX] # remove EOS if exists"
      ],
      "metadata": {
        "id": "A-5lYTktaSTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        \n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        \n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \n",
        "        #x = [batch size, len, hid dim]\n",
        "        #mask = [batch size, 1, len, len]\n",
        "        \n",
        "        #multi attention, skip and then norm\n",
        "        _x, attention = self.self_attention(x, x, x, mask)\n",
        "        x = self.self_attn_layer_norm(x + self.dropout(_x))\n",
        "        #x = [batch size, len, hid dim]\n",
        "        #attention = [batch size, n heads, len, len]\n",
        "    \n",
        "        #positionwise feedforward\n",
        "        _x = self.positionwise_feedforward(x)\n",
        "        x = self.ff_layer_norm(x + self.dropout(_x))\n",
        "        #x = [batch size, len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "metadata": {
        "id": "b1X85GkpaXWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training"
      ],
      "metadata": {
        "id": "mFSEhx4aaalM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "hid_dim    = 256                \n",
        "dec_layers = 3               \n",
        "dec_heads  = 8\n",
        "dec_pf_dim = 512\n",
        "dec_dropout = 0.1     \n",
        "lr = 1e-3                     \n",
        "model = Decoder(vocab_size, hid_dim, dec_layers, dec_heads, dec_pf_dim, dec_dropout, device, PAD_IDX).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnZJz4TRaa9E",
        "outputId": "213cbb8d-a11d-4f8d-856d-69930bb6d37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 315,120,706 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    #data #[batch size, bunch of tokens]\n",
        "    src    = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
        "    return src, target\n",
        "\n",
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    # data #[batch size, bunch of tokens]\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
        "    num_batches = data.shape[-1]\n",
        "        \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, _ = model(src)               \n",
        "\n",
        "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches\n",
        "\n",
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "    \n",
        "    decoded_batch_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            #target = [batch size, dec len]\n",
        "\n",
        "            batch_size= src.shape[0]\n",
        "            prediction, _ = model(src)\n",
        "            #prediction = [batch size, dec len, output_dim]\n",
        "            \n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "            \n",
        "    #decoding using beam_search as example (you don't need to put here, because beam_search is for intference)\n",
        "    decoded_batch = model.beam_decode()\n",
        "    print(\"Sample beam sentence: \" + \" \".join(decoded_batch))\n",
        "            \n",
        "    return epoch_loss / num_batches"
      ],
      "metadata": {
        "id": "3JOFuMROahoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5\n",
        "seq_len  = 50 #<----decoding length\n",
        "clip    = 0.25\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_data, optimizer, criterion, \n",
        "                batch_size, seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "                seq_len, device)\n",
        "\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-val-tr_lm.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1}:')\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "f_xaUPVgaokN",
        "outputId": "af160789-bfa3-4177-9e5f-81f12ad01b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d64494b92b60>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     train_loss = train(model, train_data, optimizer, criterion, \n\u001b[0m\u001b[1;32m     11\u001b[0m                 batch_size, seq_len, clip, device)\n\u001b[1;32m     12\u001b[0m     valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
            "\u001b[0;32m<ipython-input-14-69ba2d7d7bc9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#need to reshape because criterion expects pred to be 2d and target to be 1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9e4504c13834>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#attention = [batch size, n heads, len, len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m#output = [batch size, len, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.57 GiB (GPU 0; 14.75 GiB total capacity; 1.50 GiB already allocated; 12.44 GiB free; 1.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Testing"
      ],
      "metadata": {
        "id": "RD5MWlOqa30P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best-val-tr_lm.pt',  map_location=device))\n",
        "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
        "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
      ],
      "metadata": {
        "id": "0kAbrgByatqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Real World Inference"
      ],
      "metadata": {
        "id": "Zb-UtyC3a8lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, _ = model(src)\n",
        "            \n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "            \n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
        "            \n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "            \n",
        "            #####################################################################\n",
        "            #I only do pure sampling....\n",
        "            #you may want to compare here with top-k, top-p, and beam search here\n",
        "            #####################################################################\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens\n",
        "\n",
        "prompt = 'import torch '\n",
        "max_seq_len = 30\n",
        "seed = 0\n",
        "\n",
        "#smaller the temperature, more diverse tokens but comes \n",
        "#with a tradeoff of less-make-sense sentence\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
        "                          vocab, device, seed)\n",
        "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ],
      "metadata": {
        "id": "iCyfIXOua8ET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}