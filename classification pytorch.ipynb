{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcc6a90-1e58-4c3b-8b13-cbecbf33fee8",
   "metadata": {},
   "source": [
    "# Classification using Pytorch and Torchtext\n",
    "\n",
    "- bidirectional LSTM on some news\n",
    "- some labeled data on news....\n",
    "\n",
    "- Torchtext to help us numericalize and load some data\n",
    "- Torchtext is backed by PyTorch....so naturally torchtext is quite good\n",
    "- Torchtext is not meant to replace spacy....spacy is still like better in general\n",
    "  \n",
    "- PyTorch to help us make some neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5f7fbb-fdce-49b0-a5ae-3f933a9a5df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchtext\n",
    "from torch import nn\n",
    "\n",
    "import time\n",
    "\n",
    "#1. puffer - it's outdated....\n",
    "#2. spend some money - 300 baht get collab pro\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#reproducibility \n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ce65b2-1d98-4962-94aa-b9c9901bb203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f873d2-2072-4cae-8077-ab5fbea5ebee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd5220-fef6-4f24-a0b0-ee6571454546",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n",
    "\n",
    "Make our life easy by using some ready-to-be-used dataset by torchtext\n",
    "\n",
    "- in your assignment, i will ask you to use penn treebank....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e7bcaf-c276-4fdd-b8f0-b4f32ccd97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you are using puffer\n",
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "# from torchtext.datasets import AG_NEWS\n",
    "# train, test = AG_NEWS()\n",
    "\n",
    "import csv\n",
    "data_dp_train = open(\"train.txt\", encoding=\"utf-8\")\n",
    "data_dp_test = open(\"test.txt\", encoding=\"utf-8\")\n",
    "train = csv.reader(data_dp_train, skipinitialspace=True)\n",
    "test = csv.reader(data_dp_test, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e540964-c348-4b3d-948e-c01d79c40bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x7f8dfae30430>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040906c-4811-44c4-82f5-89d381f8bff0",
   "metadata": {},
   "source": [
    "## 2. EDA - exploratory data analysis\n",
    "- check common words\n",
    "- look at some random sample....how it looks, so that we can design proper neural network\n",
    "- visualize statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b0a6f7-b156-4ff2-8faa-8c418c311266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 Century) (2 's)) (2 (3 new) (2 (2 ``) (2 Conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 Arnold) (2 Schwarzenegger)) (2 \",\n",
       " ')) (2 (2 Jean-Claud) (2 (2 Van) (2 Damme)))) (2 or)) (2 (2 Steven) (2 Segal))))))))))))) (2 .)))']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463204ad-59d0-4b83-b33e-ba89e4292a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"(1 (2 Frida) (2 (1 (1 (2 is) (2 n't)) (2 (2 (2 that) (2 (2 much) (2 different))) (2 (2 from) (3 (2 many) (2 (2 a) (3 (2 Hollywood) (3 romance))))))) (2 .)))\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iter(train))[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af93ab69-4b7d-4c18-83bf-9f78fcbe571f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([y for y, x in list(iter(train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a4a3a4-1d5e-4037-a25c-1f381fc70e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf77d1e-2f3e-4d36-9caa-4fa0fb32af93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x7f8dfae30430>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70c6fa9-0d4c-4c7d-8458-0c1b1a616597",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_csv.reader' object has no attribute 'random_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/83214856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#i gonna cheat a little bit, not gonna use all...my fans will work too hard....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m too_much, train, valid = train.random_split(total_length=train_size, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                             weights = {\"too_much\": 0.7, \n\u001b[1;32m      4\u001b[0m                                                        \u001b[0;34m\"smaller_train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                        \"valid\": 0.1},\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_csv.reader' object has no attribute 'random_split'"
     ]
    }
   ],
   "source": [
    "#i gonna cheat a little bit, not gonna use all...my fans will work too hard....\n",
    "too_much, train, valid = train.random_split(total_length=train_size, \n",
    "                                            weights = {\"too_much\": 0.7, \n",
    "                                                       \"smaller_train\": 0.2,\n",
    "                                                       \"valid\": 0.1},\n",
    "                                            seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188d0b6-1ea7-46ca-bb50-330ac72e04a8",
   "metadata": {},
   "source": [
    "I can't run this term although I try to recheck about installation or search in another source. I'm so sorry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570801a5-3f1b-4563-8bbe-a391ed26026f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/1251960560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_size\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_size\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid' is not defined"
     ]
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "val_size   = len(list(iter(valid)))\n",
    "test_size  = len(list(iter(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8a9d7-17e8-4430-8e62-f5313fb0edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733c1fd-2796-45d9-8ea9-25082b22b98a",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26159b4e-9684-4ddc-a7fd-f5c64c6c958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 Tokenizing\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "#check whether the tokenizer works.....\n",
    "# tokens    = tokenizer(\"Chaky likes deep learning very much and wants his \\\n",
    "                    #   student to be number 1 in Asia....\")\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b8295f-f78a-4f8e-9ad3-4009b4057eb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/1139675545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82890dc2-61c6-47cf-b824-d5f60ae883e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2 Numericalization\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data_iter):  #data_iter, e.g., train\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>',\n",
    "                                                                 '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "393131ad-6cde-41dd-b9fd-670b45a8b223",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/984126547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#if you don't the id of this word, set it unk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "vocab.set_default_index(vocab[\"<unk>\"]) #if you don't the id of this word, set it unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44085421-5f7f-424c-a974-a96e2d167890",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/1015770521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Chaky'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wants'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'his'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'student'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'be'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'number'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "vocab(['Chaky', 'wants', 'his', 'student', 'to', 'be', 'number', '1', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e026f01-5e74-495a-ad90-a6549238505e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/4264867765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_itos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "id2word = vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f37b3a31-6fd8-4fbd-93e2-97e448f20822",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/879736971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'id2word' is not defined"
     ]
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2563a0f-003e-4310-ad0c-2530a351ee1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/925682430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<bos>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "vocab(['<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6240bf5-356e-4a33-8ef9-79332569b018",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/2434904630.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd5ea3-3234-4f48-91de-a7a758ed4775",
   "metadata": {},
   "source": [
    "## 4. FastText embedding\n",
    "\n",
    "We gonna insert this embedding to the NN on the fly....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7452f6-d1b7-4132-9b00-f4b7df93d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05cb41-5450-4be6-a639-56854d8371cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72eb01-936c-43f0-a108-c76e36a95b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding.shape #(vocab size, 300) == (52k, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199b263-77f2-4600-afa3-4ef872c15b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please lookup the fasttext embedding of id 100\n",
    "fast_embedding[100][:10] #size of 300 dim of this word id 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7b354-23b8-4abb-9067-036451e16616",
   "metadata": {},
   "source": [
    "## 5. Preparing dataloader\n",
    "\n",
    "Optional - you can either make your own batch loader....\n",
    "You can use pytorch dataloader...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e6a0b-3516-401d-be07-6ca7171ab9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1  #1, 2, 3, 4 ---> 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5291f70-34fe-44f9-b137-b5ce4ee3df84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhy padding????\\n\\nin the same batch, e.g., batch size = 2\\n\\n\"chaky eat sushi\", ==> \"chaky\", \"eat\", \"sushi\" ==> 0, 22, 11, 1, 1\\n\"chaky sleep\" ==> \"chaky\", \"sleep\" ==> 0, 99, 1, 1, 1\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "why padding????\n",
    "\n",
    "in the same batch, e.g., batch size = 2\n",
    "\n",
    "\"chaky eat sushi\", ==> \"chaky\", \"eat\", \"sushi\" ==> 0, 22, 11, 1, 1\n",
    "\"chaky sleep\" ==> \"chaky\", \"sleep\" ==> 0, 99, 1, 1, 1\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d1229af-46ae-4b31-88b8-4053fda93909",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/3801212630.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m \u001b[0;31m#making each batch same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpad_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#this function gonna be called by DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence #making each batch same length\n",
    "\n",
    "pad_ix = vocab['<pad>']\n",
    "\n",
    "#this function gonna be called by DataLoader\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))  #[3, 1, 0, 2, ]\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) #[0, 44, 21, 2]\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0)) #for padding\n",
    "        \n",
    "    return torch.tensor(label_list, dtype=torch.int64), \\\n",
    "        pad_sequence(text_list, padding_value=pad_ix, batch_first=True), \\\n",
    "        torch.tensor(length_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e8af9-533b-4ffe-baf8-673ebf3f4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "val_loader   = DataLoader(valid, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "test_loader  = DataLoader(test, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6901e-deda-4729-af7d-0e8a174ab170",
   "metadata": {},
   "source": [
    "## 6. Designing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d31e44-7aca-4edf-afcd-883a3638c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, \n",
    "                 dropout):\n",
    "        #input dim = how many vocab you have\n",
    "        #emb dim = 300 --> we use fasttext\n",
    "        #padding_idx tells this lookup table to ignore, and just randomize....\n",
    "        #<unk>, <bos>, <eos>\n",
    "        self.embedding_layer = nn.Embedding(input_dim, emb_dim, padding_idx=pad_ix)\n",
    "        self.lstm            = nn.LSTM(emb_dim,\n",
    "                                       hid_dim,\n",
    "                                       num_layers = num_layers,\n",
    "                                       bidrectional = bidirectional,\n",
    "                                       dropout = dropout,  #dropout is applied between layers....\n",
    "                                       batch_first=True)\n",
    "        \n",
    "        self.fc              = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        #x: [batch size, seq len]\n",
    "        \n",
    "        embedded_x = self.embedding_layer(x)\n",
    "        #x: [batch size, seq len, emb dim]\n",
    "        \n",
    "        #pack this embedded_x in such a way that RNN knows to ignore padding....\n",
    "        #without batch_first = True; things will become [seq len, batch size, emb dim]\n",
    "        pack_embedded = nn.utils.rnn.pack_padded_sequence(embedded_x, lengths.to('cpu'),\n",
    "                                                          enforce_sorted=False,\n",
    "                                                          batch_first = True\n",
    "                                                          )\n",
    "        \n",
    "        #packed_outputs is basically all hidden states\n",
    "        #h is the last hidden state\n",
    "        #c is the last cell state\n",
    "        packed_outputs, (h, _) = self.lstm(pack_embedded)\n",
    "        \n",
    "        #h: [num_layers * num_directions, batch_size, hidden dim]\n",
    "        \n",
    "        #it happens that because packed_outputs is all hidden states....some hidden states near the end is\n",
    "        #hidden state for padding, pytorch guys help you\n",
    "        #by using this pad_packed_sequence, then all the hidden states will only be not padding....\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first = True)\n",
    "        #output: [batch size, seq len, direction * hidden sim]\n",
    "        \n",
    "        #last hidden state - concat last forward and backward states\n",
    "        last_hidden_state = torch.cat((h[-1, :, :], h[-2, :, :]), dim = 1)\n",
    "        #last_hidden_state: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        #for sentiment analysis.....what should i sent to my linear layer...\n",
    "        return self.fc(last_hidden_state)  #[batch_size, output_dim]==> [batch_size, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e8445-ad02-4f69-bd64-f18843690a64",
   "metadata": {},
   "source": [
    "I try so many times and it doesn't work, it might be come from the libraries issues or my computer . I'm so sorry for unexpected problems ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b9b67-0351-4497-96f4-ecc2bc1d4061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
